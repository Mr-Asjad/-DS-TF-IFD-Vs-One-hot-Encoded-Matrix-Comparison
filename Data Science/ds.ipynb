{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "baebd0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data from disease_features.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import streamlit as st\n",
    "df = pd.read_csv('disease_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9ac3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for Risk Factors: (25, 170)\n",
      "One-hot encoding for Symptoms: (25, 189)\n",
      "One-hot encoding for Signs: (25, 62)\n",
      "\n",
      "Sparsity Comparison:\n",
      "TF-IDF Sparsity: 92.96%\n",
      "One-hot Sparsity: 95.15%\n",
      "\n",
      "Dimensionality Comparison:\n",
      "TF-IDF features: 1020\n",
      "One-hot features: 421\n",
      "\n",
      "Matrix Statistics:\n",
      "TF-IDF:\n",
      "- Non-zero elements: 1795\n",
      "- Mean value: 0.0119\n",
      "\n",
      "One-hot:\n",
      "- Non-zero elements: 510\n",
      "- Mean value: 0.0485\n",
      "\n",
      "Feature Value Distribution:\n",
      "TF-IDF:\n",
      "- Min: 0.0239\n",
      "- Max: 0.6903\n",
      "- Mean: 0.1687\n",
      "- Median: 0.1618\n",
      "- Std Dev: 0.0743\n",
      "\n",
      "One-hot:\n",
      "- Unique values: [0 1]\n",
      "- Min: 0.0000\n",
      "- Max: 1.0000\n",
      "\n",
      "Memory Usage:\n",
      "TF-IDF: 14.02 KB\n",
      "One-hot: 41.11 KB\n",
      "\n",
      "Information Density:\n",
      "TF-IDF avg features per sample: 71.80\n",
      "One-hot avg features per sample: 20.40\n"
     ]
    }
   ],
   "source": [
    "#Task 1: TF-IDF Feature Extraction\n",
    "\n",
    "#Step 1 #########################################################################################\n",
    "columns_to_parse=['Risk Factors', 'Symptoms', 'Signs', 'Subtypes']\n",
    "\n",
    "for col in columns_to_parse:\n",
    "    df[col]=df[col].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else [])\n",
    "\n",
    "#print(df['Symptoms'][5])             #Test if Data Loaded      \n",
    "\n",
    "#Step 2 #########################################################################################\n",
    "# Create new string columns for TF-IDF processing\n",
    "df['Risk Factors_str'] = df['Risk Factors'].apply(lambda x: ' '.join(x))\n",
    "df['Symptoms_str'] = df['Symptoms'].apply(lambda x: ' '.join(x))\n",
    "df['Signs_str'] = df['Signs'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#print(df[['Risk Factors_str', 'Symptoms_str', 'Signs_str']].head(1))         #Just for Testing\n",
    "\n",
    "#Step 3 #########################################################################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizers = {}                        # Store vectorizers and resulting matrices\n",
    "tfidf_matrices = {}\n",
    "\n",
    "# Apply TF-IDF separately to each feature column\n",
    "for col in ['Risk Factors_str', 'Symptoms_str', 'Signs_str']:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    matrix = vectorizer.fit_transform(df[col])\n",
    "    \n",
    "    vectorizers[col] = vectorizer\n",
    "    tfidf_matrices[col] = matrix\n",
    "    \n",
    "    #print(f\"✅ TF-IDF done for '{col}' → Shape: {matrix.shape}\")\n",
    "\n",
    "#Step 4 #########################################################################################\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine TF-IDF matrices column-wise\n",
    "combined_tfidf = hstack([\n",
    "    tfidf_matrices['Risk Factors_str'],\n",
    "    tfidf_matrices['Symptoms_str'],\n",
    "    tfidf_matrices['Signs_str']\n",
    "])\n",
    "\n",
    "dense_matrix = combined_tfidf.toarray()\n",
    "#print(dense_matrix)                     #Checking Ohhh Got Big help                                            \n",
    "#print(\"✅ Combined TF-IDF Matrix shape:\", combined_tfidf.shape)\n",
    "\n",
    "#Step 5 #########################################################################################\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "onehot_matrices = {}\n",
    "\n",
    "for col in ['Risk Factors', 'Symptoms', 'Signs']:\n",
    "    onehot_matrix = mlb.fit_transform(df[col])\n",
    "    onehot_matrices[col] = onehot_matrix\n",
    "    print(f\"One-hot encoding for {col}: {onehot_matrix.shape}\")\n",
    "\n",
    "# Combine one-hot matrices\n",
    "combined_onehot = np.hstack([\n",
    "    onehot_matrices['Risk Factors'],\n",
    "    onehot_matrices['Symptoms'],\n",
    "    onehot_matrices['Signs']\n",
    "])\n",
    "\n",
    "# Compare sparsity\n",
    "tfidf_sparsity = 1.0 - (np.count_nonzero(dense_matrix) / dense_matrix.size)\n",
    "onehot_sparsity = 1.0 - (np.count_nonzero(combined_onehot) / combined_onehot.size)\n",
    "\n",
    "print(\"\\nSparsity Comparison:\")\n",
    "print(f\"TF-IDF Sparsity: {tfidf_sparsity:.2%}\")\n",
    "print(f\"One-hot Sparsity: {onehot_sparsity:.2%}\")\n",
    "\n",
    "# 1. Dimensionality comparison\n",
    "print(\"\\nDimensionality Comparison:\")\n",
    "print(f\"TF-IDF features: {combined_tfidf.shape[1]}\")\n",
    "print(f\"One-hot features: {combined_onehot.shape[1]}\")\n",
    "\n",
    "# 2. Density and statistics\n",
    "print(\"\\nMatrix Statistics:\")\n",
    "print(\"TF-IDF:\")\n",
    "print(f\"- Non-zero elements: {combined_tfidf.nnz}\")\n",
    "print(f\"- Mean value: {combined_tfidf.mean():.4f}\")\n",
    "\n",
    "print(\"\\nOne-hot:\")\n",
    "print(f\"- Non-zero elements: {np.count_nonzero(combined_onehot)}\")\n",
    "print(f\"- Mean value: {combined_onehot.mean():.4f}\")\n",
    "\n",
    "# 3. Feature Distribution Analysis\n",
    "from scipy import stats\n",
    "\n",
    "# For TF-IDF\n",
    "tfidf_values = combined_tfidf.data  # Get non-zero values\n",
    "print(\"\\nFeature Value Distribution:\")\n",
    "print(\"TF-IDF:\")\n",
    "print(f\"- Min: {tfidf_values.min():.4f}\")\n",
    "print(f\"- Max: {tfidf_values.max():.4f}\")\n",
    "print(f\"- Mean: {tfidf_values.mean():.4f}\")\n",
    "print(f\"- Median: {np.median(tfidf_values):.4f}\")\n",
    "print(f\"- Std Dev: {tfidf_values.std():.4f}\")\n",
    "\n",
    "# For One-hot\n",
    "onehot_values = combined_onehot.flatten()\n",
    "print(\"\\nOne-hot:\")\n",
    "print(f\"- Unique values: {np.unique(combined_onehot)}\")\n",
    "print(f\"- Min: {onehot_values.min():.4f}\")\n",
    "print(f\"- Max: {onehot_values.max():.4f}\")\n",
    "#print(f\"- Mean: {onehot_values.mean():.4f}\")                #Irrelevant as One-hot matrixes only have 0 and 1s\n",
    "#print(f\"- Median: {np.median(onehot_values):.4f}\")\n",
    "#print(f\"- Std Dev: {onehot_values.std():.4f}\")\n",
    "\n",
    "# 4. Memory usage\n",
    "print(\"\\nMemory Usage:\")\n",
    "print(f\"TF-IDF: {combined_tfidf.data.nbytes / 1024:.2f} KB\")\n",
    "print(f\"One-hot: {combined_onehot.nbytes / 1024:.2f} KB\")\n",
    "\n",
    "# 5. Information density (average features per sample)\n",
    "print(\"\\nInformation Density:\")\n",
    "print(f\"TF-IDF avg features per sample: {combined_tfidf.nnz / combined_tfidf.shape[0]:.2f}\")\n",
    "print(f\"One-hot avg features per sample: {np.count_nonzero(combined_onehot) / combined_onehot.shape[0]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc2f6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios:\n",
      "\n",
      "TF-IDF (TruncatedSVD):\n",
      "Total variance explained: 0.1315\n",
      "Component 1: 0.0089\n",
      "Component 2: 0.0656\n",
      "Component 3: 0.0570\n",
      "\n",
      "One-hot (PCA):\n",
      "Total variance explained: 0.2801\n",
      "Component 1: 0.1106\n",
      "Component 2: 0.0951\n",
      "Component 3: 0.0744\n"
     ]
    }
   ],
   "source": [
    "#Task 2: Dimensionality Reduction\n",
    "\n",
    "#Step 1 #########################################################################################\n",
    "# Apply PCA and Truncated SVD to both matrices\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# For TF-IDF matrix (use TruncatedSVD as it works with sparse matrices)\n",
    "n_components = 3  # You can try 3 as well\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "tfidf_reduced = svd.fit_transform(combined_tfidf)\n",
    "\n",
    "# For one-hot encoded matrix (use PCA)\n",
    "pca = PCA(n_components=n_components)\n",
    "onehot_reduced = pca.fit_transform(combined_onehot)\n",
    "\n",
    "# Compare explained variance ratios\n",
    "print(\"Explained Variance Ratios:\\n\")\n",
    "print(\"TF-IDF (TruncatedSVD):\")\n",
    "print(f\"Total variance explained: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "for i, ratio in enumerate(svd.explained_variance_ratio_):\n",
    "    print(f\"Component {i+1}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\nOne-hot (PCA):\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"Component {i+1}: {ratio:.4f}\")\n",
    "\n",
    "#Step 2 #########################################################################################\n",
    "# Visualize the reduced dimensions (2D plots)\n",
    "# First, let's create a disease category column for color-coding\n",
    "\n",
    "category_mapping = {\n",
    "    \"Acute Coronary Syndrome\": \"Cardiovascular\",\n",
    "    \"Adrenal Insufficiency\": \"Endocrine\",\n",
    "    \"Alzheimer\": \"Neurological\",\n",
    "    \"Aortic Dissection\": \"Cardiovascular\",\n",
    "    \"Asthma\": \"Respiratory\",\n",
    "    \"Atrial Fibrillation\": \"Cardiovascular\",\n",
    "    \"Cardiomyopathy\": \"Cardiovascular\",\n",
    "    \"COPD\": \"Respiratory\",\n",
    "    \"Diabetes\": \"Endocrine\",\n",
    "    \"Epilepsy\": \"Neurological\",\n",
    "    \"Gastritis\": \"Gastrointestinal\",\n",
    "    \"Gastro-oesophageal Reflux Disease\": \"Gastrointestinal\",\n",
    "    \"Heart Failure\": \"Cardiovascular\",\n",
    "    \"Hyperlipidemia\": \"Cardiovascular\",\n",
    "    \"Hypertension\": \"Cardiovascular\",\n",
    "    \"Migraine\": \"Neurological\",\n",
    "    \"Multiple Sclerosis\": \"Neurological\",\n",
    "    \"Peptic Ulcer Disease\": \"Gastrointestinal\",\n",
    "    \"Pituitary Disease\": \"Endocrine\",\n",
    "    \"Pneumonia\": \"Respiratory\",\n",
    "    \"Pulmonary Embolism\": \"Cardiovascular\",\n",
    "    \"Stroke\": \"Neurological\",\n",
    "    \"Thyroid Disease\": \"Endocrine\",\n",
    "    \"Tuberculosis\": \"Infectious\",\n",
    "    \"Upper Gastrointestinal Bleeding\": \"Gastrointestinal\"\n",
    "}\n",
    "\n",
    "# Assign categories based on the mapping\n",
    "df['Category'] = df['Disease'].map(category_mapping)\n",
    "\n",
    "# Create a numeric mapping for categories\n",
    "unique_categories = df['Category'].unique()\n",
    "category_to_num = {category: i for i, category in enumerate(unique_categories)}\n",
    "category_nums = df['Category'].map(category_to_num)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# Plot TF-IDF reduced dimensions\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(tfidf_reduced[:, 0], tfidf_reduced[:, 1], c=category_nums, cmap='viridis', alpha=0.8)\n",
    "plt.title('TF-IDF Vectorization (Reduced to 2D)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "\n",
    "# Create custom legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', \n",
    "                         markerfacecolor=plt.cm.viridis(category_to_num[cat]/len(category_to_num)), \n",
    "                         markersize=10, label=cat) \n",
    "                  for cat in unique_categories]\n",
    "plt.legend(handles=legend_elements, title=\"Disease Categories\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot One-hot reduced dimensions\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(onehot_reduced[:, 0], onehot_reduced[:, 1], c=category_nums, cmap='viridis', alpha=0.8)\n",
    "plt.title('One-hot Encoding (Reduced to 2D)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend(handles=legend_elements, title=\"Disease Categories\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: 3D visualization if you used 3 components\n",
    "if n_components == 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 7))\n",
    "    \n",
    "    # 3D plot for TF-IDF\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    scatter1 = ax1.scatter(tfidf_reduced[:, 0], tfidf_reduced[:, 1], tfidf_reduced[:, 2], \n",
    "                          c=category_nums, cmap='viridis', alpha=0.8)\n",
    "    ax1.set_title('TF-IDF Vectorization (3D)')\n",
    "    ax1.set_xlabel('Component 1')\n",
    "    ax1.set_ylabel('Component 2')\n",
    "    ax1.set_zlabel('Component 3')\n",
    "    \n",
    "    # Add a legend for the 3D plot\n",
    "    legend1 = ax1.legend(handles=legend_elements, title=\"Disease Categories\")\n",
    "    \n",
    "    # 3D plot for One-hot\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    scatter2 = ax2.scatter(onehot_reduced[:, 0], onehot_reduced[:, 1], onehot_reduced[:, 2], \n",
    "                          c=category_nums, cmap='viridis', alpha=0.8)\n",
    "    ax2.set_title('One-hot Encoding (3D)')\n",
    "    ax2.set_xlabel('Component 1')\n",
    "    ax2.set_ylabel('Component 2')\n",
    "    ax2.set_zlabel('Component 3')\n",
    "    \n",
    "    # Add a legend for the 3D plot\n",
    "    legend2 = ax2.legend(handles=legend_elements, title=\"Disease Categories\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Discussion: \n",
    "\n",
    "#TF-IDF Encoding:\n",
    "#The TF-IDF scatter plot shows distinctly separated clusters with less overlap among data points. Each cluster is clearly defined,\n",
    "# indicating better separability of diseases into categories.\n",
    "\n",
    "#Reason: TF-IDF probably captures the importance of individual terms within the context of their occurrence across different rows.\n",
    "# It assigns higher weights to terms that are more unique to specific rows. By emphasizing distinguishing features (risk factors, symptoms, etc.),\n",
    "#TF-IDF creates a richer representation, enabling algorithms to identify more nuanced patterns in the data.\n",
    "\n",
    "\n",
    "#One-hot Encoding:\n",
    "#The One-hot encoded scatter plot exhibits overlapping clusters where the boundaries between disease categories are less defined.\n",
    "#The data points are grouped more tightly, showing less separability.\n",
    "\n",
    "#Reason: One-hot encoding treats every feature equally without weighing its significance. It lacks the ability as only 0s and 1s\n",
    "# are used, to capture relative importance or contextual nuances, resulting in less informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab21d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. KNN Model Comparison by Normalization Method:\n",
      "\n",
      "--- KNN with None Normalization ---\n",
      "            Accuracy         F1-Score        \n",
      "Feature       TF-IDF One-hot   TF-IDF One-hot\n",
      "k Metric                                     \n",
      "3 cosine      0.6806  0.5972   0.6362  0.5574\n",
      "  euclidean   0.4861  0.2731   0.3804  0.1616\n",
      "  manhattan   0.3194  0.2731   0.1608  0.1616\n",
      "5 cosine      0.7222  0.3981   0.6778  0.3178\n",
      "  euclidean   0.4815  0.2731   0.3343  0.1505\n",
      "  manhattan   0.3611  0.2731   0.2163  0.1505\n",
      "7 cosine      0.5972  0.4444   0.5197  0.3986\n",
      "  euclidean   0.3981  0.2778   0.2509  0.1545\n",
      "  manhattan   0.3194  0.2778   0.1571  0.1545\n",
      "\n",
      "--- KNN with StandardScaler Normalization ---\n",
      "            Accuracy         F1-Score        \n",
      "Feature       TF-IDF One-hot   TF-IDF One-hot\n",
      "k Metric                                     \n",
      "3 cosine      0.6806  0.5139   0.6245  0.4730\n",
      "  euclidean   0.3194  0.2731   0.1571  0.1590\n",
      "  manhattan   0.3194  0.2731   0.1571  0.1590\n",
      "5 cosine      0.6019  0.3565   0.5111  0.2405\n",
      "  euclidean   0.3194  0.2361   0.1571  0.1167\n",
      "  manhattan   0.3194  0.2731   0.1571  0.1493\n",
      "7 cosine      0.4028  0.3981   0.2747  0.3185\n",
      "  euclidean   0.3194  0.2361   0.1571  0.1432\n",
      "  manhattan   0.3194  0.2407   0.1571  0.1378\n",
      "\n",
      "--- KNN with MinMaxScaler Normalization ---\n",
      "            Accuracy         F1-Score        \n",
      "Feature       TF-IDF One-hot   TF-IDF One-hot\n",
      "k Metric                                     \n",
      "3 cosine      0.6806  0.5556   0.6428  0.5296\n",
      "  euclidean   0.3611  0.2731   0.2056  0.1616\n",
      "  manhattan   0.3194  0.2731   0.1571  0.1616\n",
      "5 cosine      0.6435  0.4398   0.5755  0.3636\n",
      "  euclidean   0.3194  0.3148   0.1571  0.1748\n",
      "  manhattan   0.3194  0.3148   0.1571  0.1748\n",
      "7 cosine      0.4861  0.4028   0.3981  0.3292\n",
      "  euclidean   0.3194  0.2778   0.1571  0.1545\n",
      "  manhattan   0.3194  0.2778   0.1571  0.1545\n",
      "\n",
      "2. Logistic Regression Model Comparison by Normalization Method:\n",
      "               Accuracy         F1-Score         Precision          Recall  \\\n",
      "Feature         One-hot  TF-IDF  One-hot  TF-IDF   One-hot  TF-IDF One-hot   \n",
      "Normalization                                                                \n",
      "MinMaxScaler     0.4028  0.5185   0.3132  0.4123    0.2887  0.4117  0.4028   \n",
      "None             0.4028  0.5602   0.3132  0.4783    0.2887  0.5075  0.4028   \n",
      "StandardScaler   0.3981  0.6806   0.3747  0.6056    0.4537  0.5782  0.3981   \n",
      "\n",
      "                        \n",
      "Feature         TF-IDF  \n",
      "Normalization           \n",
      "MinMaxScaler    0.5185  \n",
      "None            0.5602  \n",
      "StandardScaler  0.6806  \n",
      "\n",
      "3. Best Models by Feature Type:\n",
      "                   Model Normalization  k  Metric Accuracy Precision  Recall  \\\n",
      "Best TF-IDF Model    KNN          None  5  cosine   0.7222    0.6840  0.7222   \n",
      "Best One-hot Model   KNN          None  3  cosine   0.5972    0.5887  0.5972   \n",
      "\n",
      "                   F1-Score  \n",
      "Best TF-IDF Model    0.6778  \n",
      "Best One-hot Model   0.5574  \n",
      "\n",
      "4. Top 5 Models Overall:\n",
      "                 Model Feature   Normalization    k  Metric Accuracy  \\\n",
      "1                  KNN  TF-IDF            None    5  cosine   0.7222   \n",
      "2                  KNN  TF-IDF    MinMaxScaler    3  cosine   0.6806   \n",
      "3                  KNN  TF-IDF            None    3  cosine   0.6806   \n",
      "4                  KNN  TF-IDF  StandardScaler    3  cosine   0.6806   \n",
      "5  Logistic Regression  TF-IDF  StandardScaler  N/A     N/A   0.6806   \n",
      "\n",
      "  Precision  Recall F1-Score  \n",
      "1    0.6840  0.7222   0.6778  \n",
      "2    0.6961  0.6806   0.6428  \n",
      "3    0.6609  0.6806   0.6362  \n",
      "4    0.6498  0.6806   0.6245  \n",
      "5    0.5782  0.6806   0.6056  \n",
      "\n",
      "5. Effect of k Value on KNN Performance:\n",
      "Using None normalization and cosine metric:\n",
      "        Accuracy         F1-Score        \n",
      "Feature  One-hot  TF-IDF  One-hot  TF-IDF\n",
      "k                                        \n",
      "3         0.5972  0.6806   0.5574  0.6362\n",
      "5         0.3981  0.7222   0.3178  0.6778\n",
      "7         0.4444  0.5972   0.3986  0.5197\n"
     ]
    }
   ],
   "source": [
    "#Task 3: Train KNN Models and Logistic Regression\n",
    "\n",
    "#Step 1 #########################################################################################\n",
    "# Prepare for KNN modeling with different k values and distance metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # Added for normalization\n",
    "from sklearn.pipeline import Pipeline  # Added for creating pipelines with normalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
    "\n",
    "# Define the target variable (disease categories)\n",
    "target = df['Category']\n",
    "\n",
    "# Define k values and distance metrics to test\n",
    "k_values = [3, 5, 7]\n",
    "metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "\n",
    "# Define scoring metrics for cross-validation\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "}\n",
    "\n",
    "# Create DataFrames to store results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Feature', 'Normalization', 'k', 'Metric', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "#Step 2 #########################################################################################\n",
    "# Perform 3-fold cross-validation for KNN with different configurations\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare data\n",
    "tfidf_array = combined_tfidf.toarray()\n",
    "onehot_array = combined_onehot\n",
    "\n",
    "# Define normalization methods to test\n",
    "normalizers = {\n",
    "    'None': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler()\n",
    "}\n",
    "\n",
    "# For TF-IDF features\n",
    "for norm_name, normalizer in normalizers.items():\n",
    "    for k in k_values:\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                if normalizer is None:\n",
    "                    # No normalization\n",
    "                    knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "                    cv_results = cross_validate(knn, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "                else:\n",
    "                    # With normalization using pipeline\n",
    "                    pipeline = Pipeline([\n",
    "                        ('normalizer', normalizer),\n",
    "                        ('knn', KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance'))\n",
    "                    ])\n",
    "                    cv_results = cross_validate(pipeline, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "                \n",
    "                # Store results\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'TF-IDF',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': cv_results['test_accuracy'].mean(),\n",
    "                    'Precision': cv_results['test_precision'].mean(),\n",
    "                    'Recall': cv_results['test_recall'].mean(),\n",
    "                    'F1-Score': cv_results['test_f1'].mean()\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                # Handle any errors\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'TF-IDF',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': np.nan,\n",
    "                    'Precision': np.nan,\n",
    "                    'Recall': np.nan,\n",
    "                    'F1-Score': np.nan\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# For One-hot encoded features\n",
    "for norm_name, normalizer in normalizers.items():\n",
    "    for k in k_values:\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                if normalizer is None:\n",
    "                    # No normalization\n",
    "                    knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "                    cv_results = cross_validate(knn, onehot_array, target, cv=cv, scoring=scoring)\n",
    "                else:\n",
    "                    # With normalization using pipeline\n",
    "                    pipeline = Pipeline([\n",
    "                        ('normalizer', normalizer),\n",
    "                        ('knn', KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance'))\n",
    "                    ])\n",
    "                    cv_results = cross_validate(pipeline, onehot_array, target, cv=cv, scoring=scoring)\n",
    "                \n",
    "                # Store results\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'One-hot',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': cv_results['test_accuracy'].mean(),\n",
    "                    'Precision': cv_results['test_precision'].mean(),\n",
    "                    'Recall': cv_results['test_recall'].mean(),\n",
    "                    'F1-Score': cv_results['test_f1'].mean()\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                # Handle any errors\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'One-hot',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': np.nan,\n",
    "                    'Precision': np.nan,\n",
    "                    'Recall': np.nan,\n",
    "                    'F1-Score': np.nan\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "#Step 3 #########################################################################################\n",
    "# Train Logistic Regression models on both matrices with and without normalization\n",
    "for norm_name, normalizer in normalizers.items():\n",
    "    # For TF-IDF features\n",
    "    try:\n",
    "        if normalizer is None:\n",
    "            # No normalization\n",
    "            lr = LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced')\n",
    "            lr_results = cross_validate(lr, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "        else:\n",
    "            # With normalization using pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('normalizer', normalizer),\n",
    "                ('lr', LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced'))\n",
    "            ])\n",
    "            lr_results = cross_validate(pipeline, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "        \n",
    "        # Store results\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'TF-IDF',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': lr_results['test_accuracy'].mean(),\n",
    "            'Precision': lr_results['test_precision'].mean(),\n",
    "            'Recall': lr_results['test_recall'].mean(),\n",
    "            'F1-Score': lr_results['test_f1'].mean()\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Handle any errors\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'TF-IDF',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': np.nan,\n",
    "            'Precision': np.nan,\n",
    "            'Recall': np.nan,\n",
    "            'F1-Score': np.nan\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    # For One-hot encoded features\n",
    "    try:\n",
    "        if normalizer is None:\n",
    "            # No normalization\n",
    "            lr = LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced')\n",
    "            lr_results = cross_validate(lr, onehot_array, target, cv=cv, scoring=scoring)\n",
    "        else:\n",
    "            # With normalization using pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('normalizer', normalizer),\n",
    "                ('lr', LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced'))\n",
    "            ])\n",
    "            lr_results = cross_validate(pipeline, onehot_array, target, cv=cv, scoring=scoring)\n",
    "        \n",
    "        # Store results\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'One-hot',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': lr_results['test_accuracy'].mean(),\n",
    "            'Precision': lr_results['test_precision'].mean(),\n",
    "            'Recall': lr_results['test_recall'].mean(),\n",
    "            'F1-Score': lr_results['test_f1'].mean()\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Handle any errors\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'One-hot',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': np.nan,\n",
    "            'Precision': np.nan,\n",
    "            'Recall': np.nan,\n",
    "            'F1-Score': np.nan\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "#Step 4 #########################################################################################\n",
    "# Display results in smaller, more focused tables\n",
    "# Format numeric columns to 4 decimal places\n",
    "numeric_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "for col in numeric_cols:\n",
    "    results_df[col] = results_df[col].apply(lambda x: f\"{x:.4f}\" if not pd.isna(x) else \"N/A\")\n",
    "\n",
    "# 1. Compare KNN with different normalizations\n",
    "print(\"\\n1. KNN Model Comparison by Normalization Method:\")\n",
    "for norm_name in results_df['Normalization'].unique():\n",
    "    print(f\"\\n--- KNN with {norm_name} Normalization ---\")\n",
    "    \n",
    "    # Filter data for KNN with this normalization\n",
    "    knn_norm_data = results_df[(results_df['Model'] == 'KNN') & \n",
    "                              (results_df['Normalization'] == norm_name)]\n",
    "    \n",
    "    # Create a pivot table to compare TF-IDF vs One-hot\n",
    "    pivot_table = pd.pivot_table(\n",
    "        knn_norm_data,\n",
    "        values=['Accuracy', 'F1-Score'],\n",
    "        index=['k', 'Metric'],\n",
    "        columns=['Feature'],\n",
    "        aggfunc='first'  # Just take the first value since there should be only one\n",
    "    )\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    if ('Accuracy', 'TF-IDF') in pivot_table.columns and ('Accuracy', 'One-hot') in pivot_table.columns:\n",
    "        pivot_table = pivot_table[[('Accuracy', 'TF-IDF'), ('Accuracy', 'One-hot'), \n",
    "                                  ('F1-Score', 'TF-IDF'), ('F1-Score', 'One-hot')]]\n",
    "    \n",
    "    # Display the table\n",
    "    print(pivot_table)\n",
    "\n",
    "# 2. Compare Logistic Regression with different normalizations\n",
    "print(\"\\n2. Logistic Regression Model Comparison by Normalization Method:\")\n",
    "lr_data = results_df[results_df['Model'] == 'Logistic Regression']\n",
    "\n",
    "# Create a pivot table to compare TF-IDF vs One-hot across normalizations\n",
    "lr_pivot = pd.pivot_table(\n",
    "    lr_data,\n",
    "    values=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    index=['Normalization'],\n",
    "    columns=['Feature'],\n",
    "    aggfunc='first'  # Just take the first value since there should be only one\n",
    ")\n",
    "\n",
    "# Display the table\n",
    "print(lr_pivot)\n",
    "\n",
    "# 3. Best Models by Feature Type\n",
    "print(\"\\n3. Best Models by Feature Type:\")\n",
    "# Convert F1-Score to float for finding the best models\n",
    "results_df['F1-Score_float'] = results_df['F1-Score'].apply(lambda x: float(x) if x != \"N/A\" else 0)\n",
    "\n",
    "# Find best model for TF-IDF\n",
    "best_tfidf = results_df[results_df['Feature'] == 'TF-IDF'].loc[results_df[results_df['Feature'] == 'TF-IDF']['F1-Score_float'].idxmax()]\n",
    "best_onehot = results_df[results_df['Feature'] == 'One-hot'].loc[results_df[results_df['Feature'] == 'One-hot']['F1-Score_float'].idxmax()]\n",
    "\n",
    "# Create a DataFrame with the best models\n",
    "best_models = pd.DataFrame([best_tfidf, best_onehot])\n",
    "best_models = best_models.drop('F1-Score_float', axis=1)\n",
    "best_models.index = ['Best TF-IDF Model', 'Best One-hot Model']\n",
    "\n",
    "# Display the table\n",
    "print(best_models[['Model', 'Normalization', 'k', 'Metric', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])\n",
    "\n",
    "# 4. Top 5 Models Overall\n",
    "print(\"\\n4. Top 5 Models Overall:\")\n",
    "top_models = results_df.sort_values(by='F1-Score_float', ascending=False).head(5)\n",
    "top_models = top_models.drop('F1-Score_float', axis=1)\n",
    "top_models.index = range(1, len(top_models) + 1)  # Reset index to start from 1\n",
    "print(top_models[['Model', 'Feature', 'Normalization', 'k', 'Metric', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])\n",
    "\n",
    "# 5. Effect of k Value on KNN Performance (Best Normalization and Metric)\n",
    "print(\"\\n5. Effect of k Value on KNN Performance:\")\n",
    "# Get the best normalization and metric from the top model\n",
    "best_norm = top_models[top_models['Model'] == 'KNN']['Normalization'].iloc[0] if not top_models[top_models['Model'] == 'KNN'].empty else results_df['Normalization'].iloc[0]\n",
    "best_metric = top_models[top_models['Model'] == 'KNN']['Metric'].iloc[0] if not top_models[top_models['Model'] == 'KNN'].empty else results_df['Metric'].iloc[0]\n",
    "\n",
    "# Filter data for the best normalization and metric\n",
    "k_effect_data = results_df[(results_df['Model'] == 'KNN') & \n",
    "                          (results_df['Normalization'] == best_norm) &\n",
    "                          (results_df['Metric'] == best_metric)]\n",
    "\n",
    "# Create a pivot table to compare k values\n",
    "k_pivot = pd.pivot_table(\n",
    "    k_effect_data,\n",
    "    values=['Accuracy', 'F1-Score'],\n",
    "    index=['k'],\n",
    "    columns=['Feature'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Display the table\n",
    "print(f\"Using {best_norm} normalization and {best_metric} metric:\")\n",
    "print(k_pivot)\n",
    "\n",
    "# Clean up temporary column\n",
    "results_df = results_df.drop('F1-Score_float', axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
